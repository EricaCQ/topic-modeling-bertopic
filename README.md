# Topic Modeling with BERTopic – Documentation

## Overview

This project applies **topic modeling** using [BERTopic](https://maartengr.github.io/BERTopic/) on a labeled dataset. The workflow includes preprocessing the text data, training a BERTopic model on the training set, and applying it to a validation set for topic analysis and interpretation.

---

## Dependencies

The following Python libraries are used:

- `pandas` – data manipulation
- `re` – regex for text cleaning
- `bertopic` – topic modeling
- `scikit-learn` – used internally by BERTopic
- `sentence-transformers`, `umap-learn`, `hdbscan` – used by BERTopic for embedding and clustering

---

## Dataset

Two datasets were used:

- `df_train`: training set with raw text
- `df_valid`: validation set used for inference

Both datasets are expected to have a `text` column with the original content.

---

## Preprocessing

Before training, the text data is cleaned using a custom function:

```python
import re

stop_words = set(["the", "and", "is", "to", "of", "in", "that", "for", "it", "on", "with", 
                  "as", "was", "at", "by", "an", "be", "this", "are", "from"])

def clean_text(text):
    text = text.lower()
    tokens = re.findall(r'\\b[a-z]{2,}\\b', text)
    tokens = [word for word in tokens if word not in stop_words]
    return " ".join(tokens)
The cleaned text is stored in a new column clean_text.

Topic Modeling with BERTopic

Model Training
Sempre exibir os detalhes

from bertopic import BERTopic

topic_model = BERTopic()
train_topics, train_probs = topic_model.fit_transform(train_texts)
train_texts is the list of preprocessed text from df_train.
Model Inference
Sempre exibir os detalhes

valid_topics, valid_probs = topic_model.transform(valid_texts)
valid_texts is the cleaned validation text.
Outputs and Analysis

Top keywords per topic: retrieved via topic_model.get_topic(topic_id)
General topic overview: topic_model.get_topic_info()
Visualization (optional):
Sempre exibir os detalhes

topic_model.visualize_topics()
Notes

Stopwords are removed manually using a basic list. For production use, consider integrating nltk, spaCy, or sklearn's ENGLISH_STOP_WORDS.
BERTopic internally uses UMAP and HDBSCAN for dimensionality reduction and clustering.
Author

Documentation auto-generated by ChatGPT based on notebook structure and code logic.
"""

readme_path = Path("/mnt/data/README.md")
readme_path.write_text(documentation_template.strip())

readme_path.name
